{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Overview","text":"DESCENT <p>Optimize force field parameters against reference data</p> <p> </p> <p>The <code>descent</code> framework aims to offer a modern API for training classical force field parameters (either from a  traditional format such as SMIRNOFF or from some ML model) against reference data using <code>pytorch</code>.</p> <p>This framework benefited hugely from ForceBalance, and a significant number of learning from that project, and from Lee-Ping, have influenced the design of this one.</p> <p>Warning: This code is currently experimental and under active development. If you are using this it, please be  aware that it is not guaranteed to provide correct results, the documentation and testing maybe be incomplete, and the API can change without notice.</p>"},{"location":"#installation","title":"Installation","text":"<p>This package can be installed using <code>conda</code> (or <code>mamba</code>, a faster version of <code>conda</code>):</p> <pre><code>mamba install -c conda-forge descent\n</code></pre> <p>The example notebooks further require you install <code>jupyter</code>:</p> <pre><code>mamba install -c conda-forge jupyter\n</code></pre>"},{"location":"#getting-started","title":"Getting Started","text":"<p>To get started, see the examples.</p>"},{"location":"#copyright","title":"Copyright","text":"<p>Copyright (c) 2023, Simon Boothroyd</p>"},{"location":"development/","title":"Development","text":"<p>To create a development environment, you must have <code>mamba</code> installed.</p> <p>A development conda environment can be created and activated with:</p> <pre><code>make env\nconda activate descent\n</code></pre> <p>To format the codebase:</p> <pre><code>make format\n</code></pre> <p>To run the unit tests:</p> <pre><code>make test\n</code></pre> <p>To serve the documentation locally:</p> <pre><code>mkdocs serve\n</code></pre>"},{"location":"reference/","title":"Index","text":""},{"location":"reference/#descent","title":"descent","text":"<p>descent</p> <p>Optimize classical force field parameters against reference data</p> <p>Modules:</p> <ul> <li> <code>optim</code>           \u2013            <p>Custom parameter optimizers.</p> </li> <li> <code>targets</code>           \u2013            <p>Targets to train / assess models to / against.</p> </li> <li> <code>tests</code>           \u2013            </li> <li> <code>train</code>           \u2013            <p>Helpers for training parameters.</p> </li> <li> <code>utils</code>           \u2013            <p>Utilities functions.</p> </li> </ul>"},{"location":"reference/SUMMARY/","title":"SUMMARY","text":"<ul> <li> descent<ul> <li> optim</li> <li> targets<ul> <li> dimers</li> <li> energy</li> <li> thermo</li> </ul> </li> <li> train</li> <li> utils<ul> <li> dataset</li> <li> loss</li> <li> molecule</li> <li> reporting</li> </ul> </li> </ul> </li> </ul>"},{"location":"reference/train/","title":"<code class=\"doc-symbol doc-symbol-nav doc-symbol-module\"></code> train","text":""},{"location":"reference/train/#descent.train","title":"train","text":"<p>Helpers for training parameters.</p> <p>Classes:</p> <ul> <li> <code>AttributeConfig</code>           \u2013            <p>Configuration for how a potential's attributes should be trained.</p> </li> <li> <code>ParameterConfig</code>           \u2013            <p>Configuration for how a potential's parameters should be trained.</p> </li> <li> <code>Trainable</code>           \u2013            <p>A convenient wrapper around a tensor force field that gives greater control</p> </li> </ul>"},{"location":"reference/train/#descent.train.AttributeConfig","title":"AttributeConfig  <code>pydantic-model</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration for how a potential's attributes should be trained.</p> <p>Fields:</p> <ul> <li> <code>cols</code>                 (<code>list[str]</code>)             </li> <li> <code>scales</code>                 (<code>dict[str, float]</code>)             </li> <li> <code>limits</code>                 (<code>dict[str, tuple[float | None, float | None]]</code>)             </li> </ul>"},{"location":"reference/train/#descent.train.AttributeConfig.cols","title":"cols  <code>pydantic-field</code>","text":"<pre><code>cols: list[str]\n</code></pre> <p>The parameters to train, e.g. 'k', 'length', 'epsilon'.</p>"},{"location":"reference/train/#descent.train.AttributeConfig.scales","title":"scales  <code>pydantic-field</code>","text":"<pre><code>scales: dict[str, float] = {}\n</code></pre> <p>The scales to apply to each parameter, e.g. 'k': 1.0, 'length': 1.0, 'epsilon': 1.0.</p>"},{"location":"reference/train/#descent.train.AttributeConfig.limits","title":"limits  <code>pydantic-field</code>","text":"<pre><code>limits: dict[str, tuple[float | None, float | None]] = {}\n</code></pre> <p>The min and max values to clamp each parameter within, e.g. 'k': (0.0, None), 'angle': (0.0, pi), 'epsilon': (0.0, None), where none indicates no constraint.</p>"},{"location":"reference/train/#descent.train.ParameterConfig","title":"ParameterConfig  <code>pydantic-model</code>","text":"<p>               Bases: <code>AttributeConfig</code></p> <p>Configuration for how a potential's parameters should be trained.</p> <p>Fields:</p> <ul> <li> <code>include</code>                 (<code>PotentialKeyList | None</code>)             </li> <li> <code>exclude</code>                 (<code>PotentialKeyList | None</code>)             </li> </ul>"},{"location":"reference/train/#descent.train.ParameterConfig.cols","title":"cols  <code>pydantic-field</code>","text":"<pre><code>cols: list[str]\n</code></pre> <p>The parameters to train, e.g. 'k', 'length', 'epsilon'.</p>"},{"location":"reference/train/#descent.train.ParameterConfig.scales","title":"scales  <code>pydantic-field</code>","text":"<pre><code>scales: dict[str, float] = {}\n</code></pre> <p>The scales to apply to each parameter, e.g. 'k': 1.0, 'length': 1.0, 'epsilon': 1.0.</p>"},{"location":"reference/train/#descent.train.ParameterConfig.limits","title":"limits  <code>pydantic-field</code>","text":"<pre><code>limits: dict[str, tuple[float | None, float | None]] = {}\n</code></pre> <p>The min and max values to clamp each parameter within, e.g. 'k': (0.0, None), 'angle': (0.0, pi), 'epsilon': (0.0, None), where none indicates no constraint.</p>"},{"location":"reference/train/#descent.train.ParameterConfig.include","title":"include  <code>pydantic-field</code>","text":"<pre><code>include: PotentialKeyList | None = None\n</code></pre> <p>The keys (see <code>smee.TensorPotential.parameter_keys</code> for details) corresponding to specific parameters to be trained. If <code>None</code>, all parameters will be trained.</p>"},{"location":"reference/train/#descent.train.ParameterConfig.exclude","title":"exclude  <code>pydantic-field</code>","text":"<pre><code>exclude: PotentialKeyList | None = None\n</code></pre> <p>The keys (see <code>smee.TensorPotential.parameter_keys</code> for details) corresponding to specific parameters to be excluded from training. If <code>None</code>, no parameters will be excluded.</p>"},{"location":"reference/train/#descent.train.Trainable","title":"Trainable","text":"<pre><code>Trainable(\n    force_field: TensorForceField,\n    parameters: dict[str, ParameterConfig],\n    attributes: dict[str, AttributeConfig],\n)\n</code></pre> <p>A convenient wrapper around a tensor force field that gives greater control over how parameters should be trained.</p> <p>This includes imposing limits on the values of parameters, scaling the values so parameters passed to the optimizer have similar magnitudes, and freezing parameters so they are not updated during training.</p> <pre><code>parameters: Configure which parameters to train.\nattributes: Configure which attributes to train.\n</code></pre> <p>Methods:</p> <ul> <li> <code>to_values</code>             \u2013              <p>Returns unfrozen parameter and attribute values as a flat tensor.</p> </li> <li> <code>to_force_field</code>             \u2013              <p>Returns a force field with the parameters and attributes set to the given</p> </li> <li> <code>clamp</code>             \u2013              <p>Clamps the given values to the configured min and max values.</p> </li> </ul> Source code in <code>descent/train.py</code> <pre><code>def __init__(\n    self,\n    force_field: smee.TensorForceField,\n    parameters: dict[str, ParameterConfig],\n    attributes: dict[str, AttributeConfig],\n):\n    \"\"\"\n\n    Args:\n        force_field: The force field to wrap.\n        parameters: Configure which parameters to train.\n        attributes: Configure which attributes to train.\n    \"\"\"\n    self._force_field = force_field\n\n    (\n        self._param_types,\n        param_values,\n        self._param_shapes,\n        param_unfrozen_idxs,\n        param_scales,\n        param_clamp_lower,\n        param_clamp_upper,\n    ) = self._prepare(force_field, parameters, \"parameters\")\n    (\n        self._attr_types,\n        attr_values,\n        self._attr_shapes,\n        attr_unfrozen_idxs,\n        attr_scales,\n        attr_clamp_lower,\n        attr_clamp_upper,\n    ) = self._prepare(force_field, attributes, \"attributes\")\n\n    self._values = torch.cat([param_values, attr_values])\n\n    self._unfrozen_idxs = torch.cat(\n        [param_unfrozen_idxs, attr_unfrozen_idxs + len(param_scales)]\n    ).long()\n\n    self._scales = torch.cat([param_scales, attr_scales])[self._unfrozen_idxs]\n\n    self._clamp_lower = torch.cat([param_clamp_lower, attr_clamp_lower])[\n        self._unfrozen_idxs\n    ]\n    self._clamp_upper = torch.cat([param_clamp_upper, attr_clamp_upper])[\n        self._unfrozen_idxs\n    ]\n</code></pre>"},{"location":"reference/train/#descent.train.Trainable.to_values","title":"to_values","text":"<pre><code>to_values() -&gt; Tensor\n</code></pre> <p>Returns unfrozen parameter and attribute values as a flat tensor.</p> Source code in <code>descent/train.py</code> <pre><code>@torch.no_grad()\ndef to_values(self) -&gt; torch.Tensor:\n    \"\"\"Returns unfrozen parameter and attribute values as a flat tensor.\"\"\"\n    values_flat = self.clamp(self._values[self._unfrozen_idxs] * self._scales)\n    return values_flat.detach().clone().requires_grad_()\n</code></pre>"},{"location":"reference/train/#descent.train.Trainable.to_force_field","title":"to_force_field","text":"<pre><code>to_force_field(values_flat: Tensor) -&gt; TensorForceField\n</code></pre> <p>Returns a force field with the parameters and attributes set to the given values.</p> <p>Parameters:</p> <ul> <li> <code>values_flat</code>               (<code>Tensor</code>)           \u2013            <p>A flat tensor of parameter and attribute values. See <code>to_values</code> for the expected shape and ordering.</p> </li> </ul> Source code in <code>descent/train.py</code> <pre><code>def to_force_field(self, values_flat: torch.Tensor) -&gt; smee.TensorForceField:\n    \"\"\"Returns a force field with the parameters and attributes set to the given\n    values.\n\n    Args:\n        values_flat: A flat tensor of parameter and attribute values. See\n            ``to_values`` for the expected shape and ordering.\n    \"\"\"\n    potentials = self._force_field.potentials_by_type\n\n    values = self._values.detach().clone()\n    values[self._unfrozen_idxs] = (values_flat / self._scales).clamp(\n        min=self._clamp_lower, max=self._clamp_upper\n    )\n    values = _unflatten_tensors(values, self._param_shapes + self._attr_shapes)\n\n    params = values[: len(self._param_shapes)]\n\n    for potential_type, param in zip(self._param_types, params, strict=True):\n        potentials[potential_type].parameters = param\n\n    attrs = values[len(self._param_shapes) :]\n\n    for potential_type, attr in zip(self._attr_types, attrs, strict=True):\n        potentials[potential_type].attributes = attr\n\n    return self._force_field\n</code></pre>"},{"location":"reference/train/#descent.train.Trainable.clamp","title":"clamp","text":"<pre><code>clamp(values_flat: Tensor) -&gt; Tensor\n</code></pre> <p>Clamps the given values to the configured min and max values.</p> Source code in <code>descent/train.py</code> <pre><code>@torch.no_grad()\ndef clamp(self, values_flat: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"Clamps the given values to the configured min and max values.\"\"\"\n    return (values_flat / self._scales).clamp(\n        min=self._clamp_lower, max=self._clamp_upper\n    ) * self._scales\n</code></pre>"},{"location":"reference/optim/","title":"<code class=\"doc-symbol doc-symbol-nav doc-symbol-module\"></code> optim","text":""},{"location":"reference/optim/#descent.optim","title":"optim","text":"<p>Custom parameter optimizers.</p> <p>Classes:</p> <ul> <li> <code>LevenbergMarquardtConfig</code>           \u2013            <p>Configuration for the Levenberg-Marquardt optimizer.</p> </li> </ul> <p>Functions:</p> <ul> <li> <code>levenberg_marquardt</code>             \u2013              <p>Optimize a given set of parameters using the Levenberg-Marquardt algorithm.</p> </li> </ul>"},{"location":"reference/optim/#descent.optim.LevenbergMarquardtConfig","title":"LevenbergMarquardtConfig  <code>pydantic-model</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration for the Levenberg-Marquardt optimizer.</p>"},{"location":"reference/optim/#descent.optim.levenberg_marquardt","title":"levenberg_marquardt","text":"<pre><code>levenberg_marquardt(\n    x: Tensor,\n    config: LevenbergMarquardtConfig,\n    closure_fn: ClosureFn,\n    correct_fn: CorrectFn | None = None,\n    report_fn: ReportFn | None = None,\n) -&gt; Tensor\n</code></pre> <p>Optimize a given set of parameters using the Levenberg-Marquardt algorithm.</p> Notes <ul> <li>This optimizer assumes a least-square loss function.</li> <li>This is a reimplementation of the Levenberg-Marquardt optimizer from the   ForceBalance package, and so may differ from a standard implementation.</li> </ul> <p>Parameters:</p> <ul> <li> <code>x</code>               (<code>Tensor</code>)           \u2013            <p>The initial guess of the parameters with <code>shape=(n,)</code>.</p> </li> <li> <code>config</code>               (<code>LevenbergMarquardtConfig</code>)           \u2013            <p>The optimizer config.</p> </li> <li> <code>closure_fn</code>               (<code>ClosureFn</code>)           \u2013            <p>A function that computes the loss (<code>shape=()</code>), its gradient (<code>shape=(n,)</code>), and hessian (<code>shape=(n, n)</code>). It should accept as arguments the current parameter tensor, and two booleans indicating whether the gradient and hessian are required.</p> </li> <li> <code>correct_fn</code>               (<code>CorrectFn | None</code>, default:                   <code>None</code> )           \u2013            <p>A function that can be used to correct the parameters after each step is taken and before the new loss is computed. This may include, for example, ensuring that vdW parameters are all positive. It should accept as arguments the current parameter tensor and return the corrected parameter tensor.</p> </li> <li> <code>report_fn</code>               (<code>ReportFn | None</code>, default:                   <code>None</code> )           \u2013            <p>An optional function that should be called at the end of every step. This can be used to report the current state of the optimization. It should accept as arguments the step number, the current parameter tensor the loss, gradient and hessian, the step 'quality', and a bool indicating whether the step was accepted or rejected.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Tensor</code>           \u2013            <p>The parameters that minimize the loss.</p> </li> </ul> Source code in <code>descent/optim/_lm.py</code> <pre><code>@torch.no_grad()\ndef levenberg_marquardt(\n    x: torch.Tensor,\n    config: LevenbergMarquardtConfig,\n    closure_fn: ClosureFn,\n    correct_fn: CorrectFn | None = None,\n    report_fn: ReportFn | None = None,\n) -&gt; torch.Tensor:\n    \"\"\"Optimize a given set of parameters using the Levenberg-Marquardt algorithm.\n\n    Notes:\n        * This optimizer assumes a least-square loss function.\n        * This is a reimplementation of the Levenberg-Marquardt optimizer from the\n          ForceBalance package, and so may differ from a standard implementation.\n\n    Args:\n        x: The initial guess of the parameters with ``shape=(n,)``.\n        config: The optimizer config.\n        closure_fn: A function that computes the loss (``shape=()``), its\n            gradient (``shape=(n,)``), and hessian (``shape=(n, n)``). It should\n            accept as arguments the current parameter tensor, and two booleans\n            indicating whether the gradient and hessian are required.\n        correct_fn: A function that can be used to correct the parameters after\n            each step is taken and before the new loss is computed. This may\n            include, for example, ensuring that vdW parameters are all positive.\n            It should accept as arguments the current parameter tensor and return\n            the corrected parameter tensor.\n        report_fn: An optional function that should be called at the end of every\n            step. This can be used to report the current state of the optimization.\n            It should accept as arguments the step number, the current parameter tensor\n            the loss, gradient and hessian, the step 'quality', and a bool indicating\n            whether the step was accepted or rejected.\n\n    Returns:\n        The parameters that minimize the loss.\n    \"\"\"\n\n    x = x.clone().detach().requires_grad_(x.requires_grad)\n\n    correct_fn = correct_fn if correct_fn is not None else lambda y: y\n    closure_fn = torch.enable_grad()(closure_fn)\n\n    report_fn = report_fn if report_fn is not None else lambda *_, **__: None\n\n    closure_prev = closure_fn(x, True, True)\n    trust_radius = torch.tensor(config.trust_radius).to(x.device)\n\n    loss_history = []\n    has_converged = False\n\n    best_x, best_loss = x.clone(), closure_prev[0]\n\n    for step in range(config.max_steps):\n        loss_prev, gradient_prev, hessian_prev = closure_prev\n\n        dx, expected_improvement, damping_adjusted, damping_factor = _step(\n            gradient_prev, hessian_prev, trust_radius, config\n        )\n\n        if config.mode.lower() == _HESSIAN_SEARCH:\n            dx, expected_improvement = _hessian_diagonal_search(\n                x,\n                closure_prev,\n                closure_fn,\n                correct_fn,\n                damping_factor,\n                trust_radius,\n                config,\n            )\n\n        dx_norm = torch.linalg.norm(dx)\n        _LOGGER.info(f\"{config.mode} step found (length {dx_norm:.4e})\")\n\n        x_next = correct_fn(x + dx).requires_grad_(x.requires_grad)\n\n        loss, gradient, hessian = closure_fn(x_next, True, True)\n        loss_delta = loss - loss_prev\n\n        step_quality = loss_delta / expected_improvement\n        accept_step = True\n\n        if loss &gt; (loss_prev + config.error_tolerance):\n            # reject the 'bad' step and try again from where we were\n            loss, gradient, hessian = (loss_prev, gradient_prev, hessian_prev)\n            trust_radius = _reduce_trust_radius(dx_norm, config)\n\n            accept_step = False\n        elif config.mode.lower() == _ADAPTIVE:\n            # this was a 'good' step - we can maybe increase the trust radius\n            trust_radius = _update_trust_radius(\n                dx_norm, step_quality, trust_radius, damping_adjusted, config\n            )\n\n        if accept_step:\n            x.data.copy_(x_next.data)\n            loss_history.append(loss.detach().cpu().clone())\n\n        if loss &lt; best_loss:\n            best_x, best_loss = x.clone(), loss.detach().clone()\n\n        closure_prev = (loss, gradient, hessian)\n\n        report_fn(step, x, loss, gradient, hessian, step_quality, accept_step)\n        _LOGGER.info(f\"step={step} loss={loss.detach().cpu().item():.4e}\")\n\n        if _has_converged(dx, loss_history, gradient, step_quality, config):\n            _LOGGER.info(f\"optimization has converged after {step + 1} steps.\")\n            has_converged = True\n\n            break\n\n    if not has_converged:\n        _LOGGER.info(f\"optimization has not converged after {config.max_steps} steps.\")\n\n    return best_x\n</code></pre>"},{"location":"reference/targets/","title":"Index","text":""},{"location":"reference/targets/#descent.targets","title":"targets","text":"<p>Targets to train / assess models to / against.</p> <p>Modules:</p> <ul> <li> <code>dimers</code>           \u2013            <p>Train against dimer energies.</p> </li> <li> <code>energy</code>           \u2013            <p>Train against relative energies and forces.</p> </li> <li> <code>thermo</code>           \u2013            <p>Train against thermodynamic properties.</p> </li> </ul>"},{"location":"reference/targets/dimers/","title":"<code class=\"doc-symbol doc-symbol-nav doc-symbol-module\"></code> dimers","text":""},{"location":"reference/targets/dimers/#descent.targets.dimers","title":"dimers","text":"<p>Train against dimer energies.</p> <p>Classes:</p> <ul> <li> <code>Dimer</code>           \u2013            <p>Represents a single experimental data point.</p> </li> </ul> <p>Functions:</p> <ul> <li> <code>create_dataset</code>             \u2013              <p>Create a dataset from a list of existing dimers.</p> </li> <li> <code>create_from_des</code>             \u2013              <p>Create a dataset from a DESXXX dimer set.</p> </li> <li> <code>extract_smiles</code>             \u2013              <p>Return a list of unique SMILES strings in the dataset.</p> </li> <li> <code>compute_dimer_energy</code>             \u2013              <p>Compute the energy of a dimer in a series of conformers.</p> </li> <li> <code>predict</code>             \u2013              <p>Predict the energies of each dimer in the dataset.</p> </li> <li> <code>default_closure</code>             \u2013              <p>Return a default closure function for training against dimer energies.</p> </li> <li> <code>report</code>             \u2013              <p>Generate a report comparing the predicted and reference energies of each dimer.</p> </li> </ul>"},{"location":"reference/targets/dimers/#descent.targets.dimers.Dimer","title":"Dimer","text":"<p>               Bases: <code>TypedDict</code></p> <p>Represents a single experimental data point.</p>"},{"location":"reference/targets/dimers/#descent.targets.dimers.create_dataset","title":"create_dataset","text":"<pre><code>create_dataset(dimers: list[Dimer]) -&gt; Dataset\n</code></pre> <p>Create a dataset from a list of existing dimers.</p> <p>Parameters:</p> <ul> <li> <code>dimers</code>               (<code>list[Dimer]</code>)           \u2013            <p>The dimers to create the dataset from.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Dataset</code>           \u2013            <p>The created dataset.</p> </li> </ul> Source code in <code>descent/targets/dimers.py</code> <pre><code>def create_dataset(dimers: list[Dimer]) -&gt; datasets.Dataset:\n    \"\"\"Create a dataset from a list of existing dimers.\n\n    Args:\n        dimers: The dimers to create the dataset from.\n\n    Returns:\n        The created dataset.\n    \"\"\"\n\n    table = pyarrow.Table.from_pylist(\n        [\n            {\n                \"smiles_a\": dimer[\"smiles_a\"],\n                \"smiles_b\": dimer[\"smiles_b\"],\n                \"coords\": torch.tensor(dimer[\"coords\"]).flatten().tolist(),\n                \"energy\": torch.tensor(dimer[\"energy\"]).flatten().tolist(),\n                \"source\": dimer[\"source\"],\n            }\n            for dimer in dimers\n        ],\n        schema=DATA_SCHEMA,\n    )\n    # TODO: validate rows\n    dataset = datasets.Dataset(datasets.table.InMemoryTable(table))\n    dataset.set_format(\"torch\")\n\n    return dataset\n</code></pre>"},{"location":"reference/targets/dimers/#descent.targets.dimers.create_from_des","title":"create_from_des","text":"<pre><code>create_from_des(\n    data_dir: Path, energy_fn: EnergyFn\n) -&gt; Dataset\n</code></pre> <p>Create a dataset from a DESXXX dimer set.</p> <p>Parameters:</p> <ul> <li> <code>data_dir</code>               (<code>Path</code>)           \u2013            <p>The path to the DESXXX directory.</p> </li> <li> <code>energy_fn</code>               (<code>EnergyFn</code>)           \u2013            <p>A function which computes the reference energy of a dimer. This should take as input a pandas DataFrame containing the metadata for a given group, a tuple of geometry IDs, and a tensor of coordinates with <code>shape=(n_dimers, n_atoms, 3)</code>. It should return a tensor of energies with <code>shape=(n_dimers,)</code> and units of [kcal/mol].</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Dataset</code>           \u2013            <p>The created dataset.</p> </li> </ul> Source code in <code>descent/targets/dimers.py</code> <pre><code>def create_from_des(\n    data_dir: pathlib.Path,\n    energy_fn: EnergyFn,\n) -&gt; datasets.Dataset:\n    \"\"\"Create a dataset from a DESXXX dimer set.\n\n    Args:\n        data_dir: The path to the DESXXX directory.\n        energy_fn: A function which computes the reference energy of a dimer. This\n            should take as input a pandas DataFrame containing the metadata for a\n            given group, a tuple of geometry IDs, and a tensor of coordinates with\n            ``shape=(n_dimers, n_atoms, 3)``. It should return a tensor of energies\n            with ``shape=(n_dimers,)`` and units of [kcal/mol].\n\n    Returns:\n        The created dataset.\n    \"\"\"\n    import pandas\n    from rdkit import Chem, RDLogger\n\n    RDLogger.DisableLog(\"rdApp.*\")\n\n    metadata = pandas.read_csv(data_dir / f\"{data_dir.name}.csv\", index_col=False)\n\n    system_ids = metadata[\"system_id\"].unique()\n    dimers: list[Dimer] = []\n\n    for system_id in tqdm.tqdm(system_ids, desc=\"loading dimers\"):\n        system_data = metadata[metadata[\"system_id\"] == system_id]\n\n        group_ids = metadata[metadata[\"system_id\"] == system_id][\"group_id\"].unique()\n\n        for group_id in group_ids:\n            group_data = system_data[system_data[\"group_id\"] == group_id]\n            group_orig = group_data[\"group_orig\"].unique()[0]\n\n            geometry_ids = tuple(group_data[\"geom_id\"].values)\n\n            dimer_example = Chem.MolFromMolFile(\n                f\"{data_dir}/geometries/{system_id}/DES{group_orig}_{geometry_ids[0]}.mol\",\n                removeHs=False,\n            )\n            mol_a, mol_b = Chem.GetMolFrags(dimer_example, asMols=True)\n\n            smiles_a = descent.utils.molecule.mol_to_smiles(mol_a, False)\n            smiles_b = descent.utils.molecule.mol_to_smiles(mol_b, False)\n\n            source = (\n                f\"{data_dir.name} system={system_id} orig={group_orig} group={group_id}\"\n            )\n\n            coords_raw = [\n                Chem.MolFromMolFile(\n                    f\"{data_dir}/geometries/{system_id}/DES{group_orig}_{geometry_id}.mol\",\n                    removeHs=False,\n                )\n                .GetConformer()\n                .GetPositions()\n                .tolist()\n                for geometry_id in geometry_ids\n            ]\n\n            coords = torch.tensor(coords_raw)\n            energy = energy_fn(group_data, geometry_ids, coords)\n\n            dimer = {\n                \"smiles_a\": smiles_a,\n                \"smiles_b\": smiles_b,\n                \"coords\": coords,\n                \"energy\": energy,\n                \"source\": source,\n            }\n            dimers.append(dimer)\n\n    RDLogger.EnableLog(\"rdApp.*\")\n\n    return create_dataset(dimers)\n</code></pre>"},{"location":"reference/targets/dimers/#descent.targets.dimers.extract_smiles","title":"extract_smiles","text":"<pre><code>extract_smiles(dataset: Dataset) -&gt; list[str]\n</code></pre> <p>Return a list of unique SMILES strings in the dataset.</p> <p>Parameters:</p> <ul> <li> <code>dataset</code>               (<code>Dataset</code>)           \u2013            <p>The dataset to extract the SMILES strings from.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>list[str]</code>           \u2013            <p>The list of unique SMILES strings.</p> </li> </ul> Source code in <code>descent/targets/dimers.py</code> <pre><code>def extract_smiles(dataset: datasets.Dataset) -&gt; list[str]:\n    \"\"\"Return a list of unique SMILES strings in the dataset.\n\n    Args:\n        dataset: The dataset to extract the SMILES strings from.\n\n    Returns:\n        The list of unique SMILES strings.\n    \"\"\"\n\n    smiles_a = dataset.unique(\"smiles_a\")\n    smiles_b = dataset.unique(\"smiles_b\")\n\n    return sorted({*smiles_a, *smiles_b})\n</code></pre>"},{"location":"reference/targets/dimers/#descent.targets.dimers.compute_dimer_energy","title":"compute_dimer_energy","text":"<pre><code>compute_dimer_energy(\n    topology_a: TensorTopology,\n    topology_b: TensorTopology,\n    force_field: TensorForceField,\n    coords: Tensor,\n) -&gt; Tensor\n</code></pre> <p>Compute the energy of a dimer in a series of conformers.</p> <p>Parameters:</p> <ul> <li> <code>topology_a</code>               (<code>TensorTopology</code>)           \u2013            <p>The topology of the first monomer.</p> </li> <li> <code>topology_b</code>               (<code>TensorTopology</code>)           \u2013            <p>The topology of the second monomer.</p> </li> <li> <code>force_field</code>               (<code>TensorForceField</code>)           \u2013            <p>The force field to use.</p> </li> <li> <code>coords</code>               (<code>Tensor</code>)           \u2013            <p>The coordinates of the dimer with <code>shape=(n_dimers, n_atoms, 3)</code>.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Tensor</code>           \u2013            <p>The energy [kcal/mol] of the dimer in each conformer.</p> </li> </ul> Source code in <code>descent/targets/dimers.py</code> <pre><code>def compute_dimer_energy(\n    topology_a: smee.TensorTopology,\n    topology_b: smee.TensorTopology,\n    force_field: smee.TensorForceField,\n    coords: torch.Tensor,\n) -&gt; torch.Tensor:\n    \"\"\"Compute the energy of a dimer in a series of conformers.\n\n    Args:\n        topology_a: The topology of the first monomer.\n        topology_b: The topology of the second monomer.\n        force_field: The force field to use.\n        coords: The coordinates of the dimer with ``shape=(n_dimers, n_atoms, 3)``.\n\n    Returns:\n        The energy [kcal/mol] of the dimer in each conformer.\n    \"\"\"\n    dimer = smee.TensorSystem([topology_a, topology_b], [1, 1], False)\n\n    coords_a = coords[:, : topology_a.n_atoms, :]\n\n    if topology_a.v_sites is not None:\n        coords_a = smee.geometry.add_v_site_coords(\n            topology_a.v_sites, coords_a, force_field\n        )\n\n    coords_b = coords[:, topology_a.n_atoms :, :]\n\n    if topology_b.v_sites is not None:\n        coords_b = smee.geometry.add_v_site_coords(\n            topology_b.v_sites, coords_b, force_field\n        )\n\n    coords = torch.cat([coords_a, coords_b], dim=1)\n\n    energy_dimer = smee.compute_energy(dimer, force_field, coords)\n\n    energy_a = smee.compute_energy(topology_a, force_field, coords_a)\n    energy_b = smee.compute_energy(topology_b, force_field, coords_b)\n\n    return energy_dimer - energy_a - energy_b\n</code></pre>"},{"location":"reference/targets/dimers/#descent.targets.dimers.predict","title":"predict","text":"<pre><code>predict(\n    dataset: Dataset,\n    force_field: TensorForceField,\n    topologies: dict[str, TensorTopology],\n) -&gt; tuple[Tensor, Tensor]\n</code></pre> <p>Predict the energies of each dimer in the dataset.</p> <p>Parameters:</p> <ul> <li> <code>dataset</code>               (<code>Dataset</code>)           \u2013            <p>The dataset to predict the energies of.</p> </li> <li> <code>force_field</code>               (<code>TensorForceField</code>)           \u2013            <p>The force field to use.</p> </li> <li> <code>topologies</code>               (<code>dict[str, TensorTopology]</code>)           \u2013            <p>The topologies of each monomer. Each key should be a fully mapped SMILES string.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>tuple[Tensor, Tensor]</code>           \u2013            <p>The reference and predicted energies [kcal/mol] of each dimer, each with <code>shape=(n_dimers * n_conf_per_dimer,)</code>.</p> </li> </ul> Source code in <code>descent/targets/dimers.py</code> <pre><code>def predict(\n    dataset: datasets.Dataset,\n    force_field: smee.TensorForceField,\n    topologies: dict[str, smee.TensorTopology],\n) -&gt; tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"Predict the energies of each dimer in the dataset.\n\n    Args:\n        dataset: The dataset to predict the energies of.\n        force_field: The force field to use.\n        topologies: The topologies of each monomer. Each key should be a fully\n            mapped SMILES string.\n\n    Returns:\n        The reference and predicted energies [kcal/mol] of each dimer, each with\n        ``shape=(n_dimers * n_conf_per_dimer,)``.\n    \"\"\"\n\n    reference, predicted = zip(\n        *[\n            _predict(dimer, force_field, topologies)\n            for dimer in descent.utils.dataset.iter_dataset(dataset)\n        ],\n        strict=True,\n    )\n    return torch.cat(reference), torch.cat(predicted)\n</code></pre>"},{"location":"reference/targets/dimers/#descent.targets.dimers.default_closure","title":"default_closure","text":"<pre><code>default_closure(\n    trainable: Trainable,\n    topologies: dict[str, TensorTopology],\n    dataset: Dataset,\n)\n</code></pre> <p>Return a default closure function for training against dimer energies.</p> <p>Parameters:</p> <ul> <li> <code>trainable</code>               (<code>Trainable</code>)           \u2013            <p>The wrapper around trainable parameters.</p> </li> <li> <code>topologies</code>               (<code>dict[str, TensorTopology]</code>)           \u2013            <p>The topologies of the molecules present in the dataset, with keys of mapped SMILES patterns.</p> </li> <li> <code>dataset</code>               (<code>Dataset</code>)           \u2013            <p>The dataset to train against.</p> </li> </ul> <p>Returns:</p> <ul> <li>           \u2013            <p>The default closure function.</p> </li> </ul> Source code in <code>descent/targets/dimers.py</code> <pre><code>def default_closure(\n    trainable: \"descent.train.Trainable\",\n    topologies: dict[str, smee.TensorTopology],\n    dataset: datasets.Dataset,\n):\n    \"\"\"Return a default closure function for training against dimer energies.\n\n    Args:\n        trainable: The wrapper around trainable parameters.\n        topologies: The topologies of the molecules present in the dataset, with keys\n            of mapped SMILES patterns.\n        dataset: The dataset to train against.\n\n    Returns:\n        The default closure function.\n    \"\"\"\n\n    def loss_fn(_x: torch.Tensor) -&gt; torch.Tensor:\n        y_ref, y_pred = descent.targets.dimers.predict(\n            dataset, trainable.to_force_field(_x), topologies\n        )\n        return ((y_pred - y_ref) ** 2).sum()\n\n    return descent.utils.loss.to_closure(loss_fn)\n</code></pre>"},{"location":"reference/targets/dimers/#descent.targets.dimers.report","title":"report","text":"<pre><code>report(\n    dataset: Dataset,\n    force_fields: dict[str, TensorForceField],\n    topologies: dict[str, TensorTopology],\n    output_path: Path,\n)\n</code></pre> <p>Generate a report comparing the predicted and reference energies of each dimer.</p> <p>Parameters:</p> <ul> <li> <code>dataset</code>               (<code>Dataset</code>)           \u2013            <p>The dataset to generate the report for.</p> </li> <li> <code>force_fields</code>               (<code>dict[str, TensorForceField]</code>)           \u2013            <p>The force fields to use to predict the energies.</p> </li> <li> <code>topologies</code>               (<code>dict[str, TensorTopology]</code>)           \u2013            <p>The topologies of each monomer. Each key should be a fully mapped SMILES string.</p> </li> <li> <code>output_path</code>               (<code>Path</code>)           \u2013            <p>The path to write the report to.</p> </li> </ul> Source code in <code>descent/targets/dimers.py</code> <pre><code>def report(\n    dataset: datasets.Dataset,\n    force_fields: dict[str, smee.TensorForceField],\n    topologies: dict[str, smee.TensorTopology],\n    output_path: pathlib.Path,\n):\n    \"\"\"Generate a report comparing the predicted and reference energies of each dimer.\n\n    Args:\n        dataset: The dataset to generate the report for.\n        force_fields: The force fields to use to predict the energies.\n        topologies: The topologies of each monomer. Each key should be a fully\n            mapped SMILES string.\n        output_path: The path to write the report to.\n    \"\"\"\n    import pandas\n\n    rows = []\n\n    delta_sqr_total = {\n        force_field_name: torch.zeros(1) for force_field_name in force_fields\n    }\n    delta_sqr_count = 0\n\n    for dimer in descent.utils.dataset.iter_dataset(dataset):\n        energies = {\"ref\": dimer[\"energy\"]}\n        energies.update(\n            (force_field_name, _predict(dimer, force_field, topologies)[1])\n            for force_field_name, force_field in force_fields.items()\n        )\n\n        mol_img = descent.utils.reporting.mols_to_img(\n            dimer[\"smiles_a\"], dimer[\"smiles_b\"]\n        )\n        data_row = {\"Dimer\": mol_img, \"Energy [kcal/mol]\": _plot_energies(energies)}\n\n        for force_field_name in force_fields:\n            delta_sqr = ((energies[\"ref\"] - energies[force_field_name]) ** 2).sum()\n            delta_sqr_total[force_field_name] += delta_sqr\n\n            rmse = torch.sqrt(delta_sqr / len(energies[\"ref\"]))\n            data_row[f\"RMSE {force_field_name}\"] = rmse.item()\n\n        data_row[\"Source\"] = dimer[\"source\"]\n\n        delta_sqr_count += len(energies[\"ref\"])\n\n        rows.append(data_row)\n\n    rmse_total_rows = [\n        {\n            \"Force Field\": force_field_name,\n            \"RMSE [kcal/mol]\": torch.sqrt(\n                delta_sqr_total[force_field_name].sum() / delta_sqr_count\n            ).item(),\n        }\n        for force_field_name in force_fields\n    ]\n\n    import bokeh.models.widgets.tables\n    import panel\n\n    data_full = pandas.DataFrame(rows)\n    data_stats = pandas.DataFrame(rmse_total_rows)\n\n    rmse_format = bokeh.models.widgets.tables.NumberFormatter(format=\"0.0000\")\n\n    formatters_stats = {\n        col: rmse_format for col in data_stats.columns if col.startswith(\"RMSE\")\n    }\n    formatters_full = {\n        **{col: \"html\" for col in [\"Dimer\", \"Energy [kcal/mol]\"]},\n        **{col: rmse_format for col in data_full.columns if col.startswith(\"RMSE\")},\n    }\n\n    layout = panel.Column(\n        \"## Statistics\",\n        panel.widgets.Tabulator(\n            pandas.DataFrame(rmse_total_rows),\n            show_index=False,\n            selectable=False,\n            disabled=True,\n            formatters=formatters_stats,\n            configuration={\"columnDefaults\": {\"headerSort\": False}},\n        ),\n        \"## Energies\",\n        panel.widgets.Tabulator(\n            data_full,\n            show_index=False,\n            selectable=False,\n            disabled=True,\n            formatters=formatters_full,\n            configuration={\"rowHeight\": 400},\n        ),\n        sizing_mode=\"stretch_width\",\n        scroll=True,\n    )\n\n    output_path.parent.mkdir(parents=True, exist_ok=True)\n    layout.save(output_path, title=\"Dimers\", embed=True)\n</code></pre>"},{"location":"reference/targets/energy/","title":"<code class=\"doc-symbol doc-symbol-nav doc-symbol-module\"></code> energy","text":""},{"location":"reference/targets/energy/#descent.targets.energy","title":"energy","text":"<p>Train against relative energies and forces.</p> <p>Classes:</p> <ul> <li> <code>Entry</code>           \u2013            <p>Represents a set of reference energies and forces.</p> </li> </ul> <p>Functions:</p> <ul> <li> <code>create_dataset</code>             \u2013              <p>Create a dataset from a list of existing entries.</p> </li> <li> <code>extract_smiles</code>             \u2013              <p>Return a list of unique SMILES strings in the dataset.</p> </li> <li> <code>predict</code>             \u2013              <p>Predict the relative energies [kcal/mol] and forces [kcal/mol/\u00c5] of a dataset.</p> </li> </ul>"},{"location":"reference/targets/energy/#descent.targets.energy.Entry","title":"Entry","text":"<p>               Bases: <code>TypedDict</code></p> <p>Represents a set of reference energies and forces.</p> <p>Attributes:</p> <ul> <li> <code>smiles</code>               (<code>str</code>)           \u2013            <p>The indexed SMILES description of the molecule the energies and forces were</p> </li> <li> <code>coords</code>               (<code>Tensor</code>)           \u2013            <p>The coordinates [\u00c5] the energies and forces were evaluated at with</p> </li> <li> <code>energy</code>               (<code>Tensor</code>)           \u2013            <p>The reference energies [kcal/mol] with <code>shape=(n_confs,)</code>.</p> </li> <li> <code>forces</code>               (<code>Tensor</code>)           \u2013            <p>The reference forces [kcal/mol/\u00c5] with <code>shape=(n_confs, n_particles, 3)</code>.</p> </li> </ul>"},{"location":"reference/targets/energy/#descent.targets.energy.Entry.smiles","title":"smiles  <code>instance-attribute</code>","text":"<pre><code>smiles: str\n</code></pre> <p>The indexed SMILES description of the molecule the energies and forces were computed for.</p>"},{"location":"reference/targets/energy/#descent.targets.energy.Entry.coords","title":"coords  <code>instance-attribute</code>","text":"<pre><code>coords: Tensor\n</code></pre> <p>The coordinates [\u00c5] the energies and forces were evaluated at with <code>shape=(n_confs, n_particles, 3)</code>.</p>"},{"location":"reference/targets/energy/#descent.targets.energy.Entry.energy","title":"energy  <code>instance-attribute</code>","text":"<pre><code>energy: Tensor\n</code></pre> <p>The reference energies [kcal/mol] with <code>shape=(n_confs,)</code>.</p>"},{"location":"reference/targets/energy/#descent.targets.energy.Entry.forces","title":"forces  <code>instance-attribute</code>","text":"<pre><code>forces: Tensor\n</code></pre> <p>The reference forces [kcal/mol/\u00c5] with <code>shape=(n_confs, n_particles, 3)</code>.</p>"},{"location":"reference/targets/energy/#descent.targets.energy.create_dataset","title":"create_dataset","text":"<pre><code>create_dataset(entries: list[Entry]) -&gt; Dataset\n</code></pre> <p>Create a dataset from a list of existing entries.</p> <p>Parameters:</p> <ul> <li> <code>entries</code>               (<code>list[Entry]</code>)           \u2013            <p>The entries to create the dataset from.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Dataset</code>           \u2013            <p>The created dataset.</p> </li> </ul> Source code in <code>descent/targets/energy.py</code> <pre><code>def create_dataset(entries: list[Entry]) -&gt; datasets.Dataset:\n    \"\"\"Create a dataset from a list of existing entries.\n\n    Args:\n        entries: The entries to create the dataset from.\n\n    Returns:\n        The created dataset.\n    \"\"\"\n\n    table = pyarrow.Table.from_pylist(\n        [\n            {\n                \"smiles\": entry[\"smiles\"],\n                \"coords\": torch.tensor(entry[\"coords\"]).flatten().tolist(),\n                \"energy\": torch.tensor(entry[\"energy\"]).flatten().tolist(),\n                \"forces\": torch.tensor(entry[\"forces\"]).flatten().tolist(),\n            }\n            for entry in entries\n        ],\n        schema=DATA_SCHEMA,\n    )\n    # TODO: validate rows\n    dataset = datasets.Dataset(datasets.table.InMemoryTable(table))\n    dataset.set_format(\"torch\")\n\n    return dataset\n</code></pre>"},{"location":"reference/targets/energy/#descent.targets.energy.extract_smiles","title":"extract_smiles","text":"<pre><code>extract_smiles(dataset: Dataset) -&gt; list[str]\n</code></pre> <p>Return a list of unique SMILES strings in the dataset.</p> <p>Parameters:</p> <ul> <li> <code>dataset</code>               (<code>Dataset</code>)           \u2013            <p>The dataset to extract the SMILES strings from.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>list[str]</code>           \u2013            <p>The list of unique SMILES strings.</p> </li> </ul> Source code in <code>descent/targets/energy.py</code> <pre><code>def extract_smiles(dataset: datasets.Dataset) -&gt; list[str]:\n    \"\"\"Return a list of unique SMILES strings in the dataset.\n\n    Args:\n        dataset: The dataset to extract the SMILES strings from.\n\n    Returns:\n        The list of unique SMILES strings.\n    \"\"\"\n    return sorted({*dataset.unique(\"smiles\")})\n</code></pre>"},{"location":"reference/targets/energy/#descent.targets.energy.predict","title":"predict","text":"<pre><code>predict(\n    dataset: Dataset,\n    force_field: TensorForceField,\n    topologies: dict[str, TensorTopology],\n    reference: Literal[\"mean\", \"min\"] = \"mean\",\n    normalize: bool = True,\n) -&gt; tuple[Tensor, Tensor, Tensor, Tensor]\n</code></pre> <p>Predict the relative energies [kcal/mol] and forces [kcal/mol/\u00c5] of a dataset.</p> <p>Parameters:</p> <ul> <li> <code>dataset</code>               (<code>Dataset</code>)           \u2013            <p>The dataset to predict the energies and forces of.</p> </li> <li> <code>force_field</code>               (<code>TensorForceField</code>)           \u2013            <p>The force field to use to predict the energies and forces.</p> </li> <li> <code>topologies</code>               (<code>dict[str, TensorTopology]</code>)           \u2013            <p>The topologies of the molecules in the dataset. Each key should be a fully indexed SMILES string.</p> </li> <li> <code>reference</code>               (<code>Literal['mean', 'min']</code>, default:                   <code>'mean'</code> )           \u2013            <p>The reference energy to compute the relative energies with respect to. This should be either the \"mean\" energy of all conformers, or the energy of the conformer with the lowest reference energy (\"min\").</p> </li> <li> <code>normalize</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to scale the relative energies by <code>1/sqrt(n_confs_i)</code> and the forces by <code>1/sqrt(n_confs_i * n_atoms_per_conf_i * 3)</code> This is useful when wanting to compute the MSE per entry.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>tuple[Tensor, Tensor, Tensor, Tensor]</code>           \u2013            <p>The predicted and reference relative energies [kcal/mol] with <code>shape=(n_confs,)</code>, and predicted and reference forces [kcal/mol/\u00c5] with <code>shape=(n_confs * n_atoms_per_conf, 3)</code>.</p> </li> </ul> Source code in <code>descent/targets/energy.py</code> <pre><code>def predict(\n    dataset: datasets.Dataset,\n    force_field: smee.TensorForceField,\n    topologies: dict[str, smee.TensorTopology],\n    reference: typing.Literal[\"mean\", \"min\"] = \"mean\",\n    normalize: bool = True,\n) -&gt; tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\n    \"\"\"Predict the relative energies [kcal/mol] and forces [kcal/mol/\u00c5] of a dataset.\n\n    Args:\n        dataset: The dataset to predict the energies and forces of.\n        force_field: The force field to use to predict the energies and forces.\n        topologies: The topologies of the molecules in the dataset. Each key should be\n            a fully indexed SMILES string.\n        reference: The reference energy to compute the relative energies with respect\n            to. This should be either the \"mean\" energy of all conformers, or the\n            energy of the conformer with the lowest reference energy (\"min\").\n        normalize: Whether to scale the relative energies by ``1/sqrt(n_confs_i)``\n            and the forces by ``1/sqrt(n_confs_i * n_atoms_per_conf_i * 3)`` This\n            is useful when wanting to compute the MSE per entry.\n\n    Returns:\n        The predicted and reference relative energies [kcal/mol] with\n        ``shape=(n_confs,)``, and predicted and reference forces [kcal/mol/\u00c5] with\n        ``shape=(n_confs * n_atoms_per_conf, 3)``.\n    \"\"\"\n    energy_ref_all, energy_pred_all = [], []\n    forces_ref_all, forces_pred_all = [], []\n\n    for entry in dataset:\n        smiles = entry[\"smiles\"]\n\n        energy_ref = entry[\"energy\"]\n        forces_ref = entry[\"forces\"].reshape(len(energy_ref), -1, 3)\n\n        coords = (\n            entry[\"coords\"]\n            .reshape(len(energy_ref), -1, 3)\n            .detach()\n            .requires_grad_(True)\n        )\n        topology = topologies[smiles]\n\n        energy_pred = smee.compute_energy(topology, force_field, coords)\n        forces_pred = torch.autograd.grad(\n            energy_pred.sum(),\n            coords,\n            create_graph=True,\n            retain_graph=True,\n            allow_unused=True,\n        )[0]\n\n        if reference.lower() == \"mean\":\n            energy_ref_0 = energy_ref.mean()\n            energy_pred_0 = energy_pred.mean()\n        elif reference.lower() == \"min\":\n            min_idx = energy_ref.argmin()\n\n            energy_ref_0 = energy_ref[min_idx]\n            energy_pred_0 = energy_pred[min_idx]\n        else:\n            raise NotImplementedError(f\"invalid reference energy {reference}\")\n\n        scale_energy, scale_forces = 1.0, 1.0\n\n        if normalize:\n            scale_energy = 1.0 / torch.sqrt(torch.tensor(energy_pred.numel()))\n            scale_forces = 1.0 / torch.sqrt(torch.tensor(forces_pred.numel()))\n\n        energy_ref_all.append(scale_energy * (energy_ref - energy_ref_0))\n        forces_ref_all.append(scale_forces * forces_ref.reshape(-1, 3))\n\n        energy_pred_all.append(scale_energy * (energy_pred - energy_pred_0))\n        forces_pred_all.append(scale_forces * forces_pred.reshape(-1, 3))\n\n    return (\n        torch.cat(energy_ref_all),\n        torch.cat(energy_pred_all),\n        torch.cat(forces_ref_all),\n        torch.cat(forces_pred_all),\n    )\n</code></pre>"},{"location":"reference/targets/thermo/","title":"<code class=\"doc-symbol doc-symbol-nav doc-symbol-module\"></code> thermo","text":""},{"location":"reference/targets/thermo/#descent.targets.thermo","title":"thermo","text":"<p>Train against thermodynamic properties.</p> <p>Classes:</p> <ul> <li> <code>DataEntry</code>           \u2013            <p>Represents a single experimental data point.</p> </li> <li> <code>SimulationKey</code>           \u2013            <p>A key used to identify a simulation.</p> </li> <li> <code>SimulationConfig</code>           \u2013            <p>Configuration for a simulation to run.</p> </li> </ul> <p>Functions:</p> <ul> <li> <code>create_dataset</code>             \u2013              <p>Create a dataset from a list of existing data points.</p> </li> <li> <code>extract_smiles</code>             \u2013              <p>Return a list of unique SMILES strings in the dataset.</p> </li> <li> <code>default_config</code>             \u2013              <p>Return a default simulation configuration for the specified phase.</p> </li> <li> <code>predict</code>             \u2013              <p>Predict the properties in a dataset using molecular simulation, or by reweighting</p> </li> <li> <code>default_closure</code>             \u2013              <p>Return a default closure function for training against thermodynamic</p> </li> </ul>"},{"location":"reference/targets/thermo/#descent.targets.thermo.DataEntry","title":"DataEntry","text":"<p>               Bases: <code>TypedDict</code></p> <p>Represents a single experimental data point.</p> <p>Attributes:</p> <ul> <li> <code>type</code>               (<code>DataType</code>)           \u2013            <p>The type of data point.</p> </li> <li> <code>smiles_a</code>               (<code>str</code>)           \u2013            <p>The SMILES definition of the first component.</p> </li> <li> <code>x_a</code>               (<code>float | None</code>)           \u2013            <p>The mole fraction of the first component. This must be set to 1.0 if the data</p> </li> <li> <code>smiles_b</code>               (<code>str | None</code>)           \u2013            <p>The SMILES definition of the second component if present.</p> </li> <li> <code>x_b</code>               (<code>float | None</code>)           \u2013            <p>The mole fraction of the second component if present.</p> </li> <li> <code>temperature</code>               (<code>float</code>)           \u2013            <p>The temperature at which the data point was measured.</p> </li> <li> <code>pressure</code>               (<code>float</code>)           \u2013            <p>The pressure at which the data point was measured.</p> </li> <li> <code>value</code>               (<code>float</code>)           \u2013            <p>The value of the data point.</p> </li> <li> <code>std</code>               (<code>float | None</code>)           \u2013            <p>The standard deviation of the data point if available.</p> </li> <li> <code>units</code>               (<code>str</code>)           \u2013            <p>The units of the data point.</p> </li> <li> <code>source</code>               (<code>str</code>)           \u2013            <p>The source of the data point.</p> </li> </ul>"},{"location":"reference/targets/thermo/#descent.targets.thermo.DataEntry.type","title":"type  <code>instance-attribute</code>","text":"<pre><code>type: DataType\n</code></pre> <p>The type of data point.</p>"},{"location":"reference/targets/thermo/#descent.targets.thermo.DataEntry.smiles_a","title":"smiles_a  <code>instance-attribute</code>","text":"<pre><code>smiles_a: str\n</code></pre> <p>The SMILES definition of the first component.</p>"},{"location":"reference/targets/thermo/#descent.targets.thermo.DataEntry.x_a","title":"x_a  <code>instance-attribute</code>","text":"<pre><code>x_a: float | None\n</code></pre> <p>The mole fraction of the first component. This must be set to 1.0 if the data</p>"},{"location":"reference/targets/thermo/#descent.targets.thermo.DataEntry.smiles_b","title":"smiles_b  <code>instance-attribute</code>","text":"<pre><code>smiles_b: str | None\n</code></pre> <p>The SMILES definition of the second component if present.</p>"},{"location":"reference/targets/thermo/#descent.targets.thermo.DataEntry.x_b","title":"x_b  <code>instance-attribute</code>","text":"<pre><code>x_b: float | None\n</code></pre> <p>The mole fraction of the second component if present.</p>"},{"location":"reference/targets/thermo/#descent.targets.thermo.DataEntry.temperature","title":"temperature  <code>instance-attribute</code>","text":"<pre><code>temperature: float\n</code></pre> <p>The temperature at which the data point was measured.</p>"},{"location":"reference/targets/thermo/#descent.targets.thermo.DataEntry.pressure","title":"pressure  <code>instance-attribute</code>","text":"<pre><code>pressure: float\n</code></pre> <p>The pressure at which the data point was measured.</p>"},{"location":"reference/targets/thermo/#descent.targets.thermo.DataEntry.value","title":"value  <code>instance-attribute</code>","text":"<pre><code>value: float\n</code></pre> <p>The value of the data point.</p>"},{"location":"reference/targets/thermo/#descent.targets.thermo.DataEntry.std","title":"std  <code>instance-attribute</code>","text":"<pre><code>std: float | None\n</code></pre> <p>The standard deviation of the data point if available.</p>"},{"location":"reference/targets/thermo/#descent.targets.thermo.DataEntry.units","title":"units  <code>instance-attribute</code>","text":"<pre><code>units: str\n</code></pre> <p>The units of the data point.</p>"},{"location":"reference/targets/thermo/#descent.targets.thermo.DataEntry.source","title":"source  <code>instance-attribute</code>","text":"<pre><code>source: str\n</code></pre> <p>The source of the data point.</p>"},{"location":"reference/targets/thermo/#descent.targets.thermo.SimulationKey","title":"SimulationKey","text":"<p>               Bases: <code>NamedTuple</code></p> <p>A key used to identify a simulation.</p> <p>Attributes:</p> <ul> <li> <code>smiles</code>               (<code>tuple[str, ...]</code>)           \u2013            <p>The SMILES definitions of the components present in the system.</p> </li> <li> <code>counts</code>               (<code>tuple[int, ...]</code>)           \u2013            <p>The number of copies of each component present in the system.</p> </li> <li> <code>temperature</code>               (<code>float</code>)           \u2013            <p>The temperature [K] at which the simulation was run.</p> </li> <li> <code>pressure</code>               (<code>float | None</code>)           \u2013            <p>The pressure [atm] at which the simulation was run.</p> </li> </ul>"},{"location":"reference/targets/thermo/#descent.targets.thermo.SimulationKey.smiles","title":"smiles  <code>instance-attribute</code>","text":"<pre><code>smiles: tuple[str, ...]\n</code></pre> <p>The SMILES definitions of the components present in the system.</p>"},{"location":"reference/targets/thermo/#descent.targets.thermo.SimulationKey.counts","title":"counts  <code>instance-attribute</code>","text":"<pre><code>counts: tuple[int, ...]\n</code></pre> <p>The number of copies of each component present in the system.</p>"},{"location":"reference/targets/thermo/#descent.targets.thermo.SimulationKey.temperature","title":"temperature  <code>instance-attribute</code>","text":"<pre><code>temperature: float\n</code></pre> <p>The temperature [K] at which the simulation was run.</p>"},{"location":"reference/targets/thermo/#descent.targets.thermo.SimulationKey.pressure","title":"pressure  <code>instance-attribute</code>","text":"<pre><code>pressure: float | None\n</code></pre> <p>The pressure [atm] at which the simulation was run.</p>"},{"location":"reference/targets/thermo/#descent.targets.thermo.SimulationConfig","title":"SimulationConfig  <code>pydantic-model</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration for a simulation to run.</p> <p>Fields:</p> <ul> <li> <code>max_mols</code>                 (<code>int</code>)             </li> <li> <code>gen_coords</code>                 (<code>GenerateCoordsConfig</code>)             </li> <li> <code>apply_hmr</code>                 (<code>bool</code>)             </li> <li> <code>equilibrate</code>                 (<code>list[MinimizationConfig | SimulationConfig]</code>)             </li> <li> <code>production</code>                 (<code>SimulationConfig</code>)             </li> <li> <code>production_frequency</code>                 (<code>int</code>)             </li> </ul>"},{"location":"reference/targets/thermo/#descent.targets.thermo.SimulationConfig.max_mols","title":"max_mols  <code>pydantic-field</code>","text":"<pre><code>max_mols: int\n</code></pre> <p>The maximum number of molecules to simulate.</p>"},{"location":"reference/targets/thermo/#descent.targets.thermo.SimulationConfig.gen_coords","title":"gen_coords  <code>pydantic-field</code>","text":"<pre><code>gen_coords: GenerateCoordsConfig\n</code></pre> <p>Configuration for generating initial coordinates.</p>"},{"location":"reference/targets/thermo/#descent.targets.thermo.SimulationConfig.apply_hmr","title":"apply_hmr  <code>pydantic-field</code>","text":"<pre><code>apply_hmr: bool = False\n</code></pre> <p>Whether to apply hydrogen mass repartitioning.</p>"},{"location":"reference/targets/thermo/#descent.targets.thermo.SimulationConfig.equilibrate","title":"equilibrate  <code>pydantic-field</code>","text":"<pre><code>equilibrate: list[MinimizationConfig | SimulationConfig]\n</code></pre> <p>Configuration for equilibration simulations.</p>"},{"location":"reference/targets/thermo/#descent.targets.thermo.SimulationConfig.production","title":"production  <code>pydantic-field</code>","text":"<pre><code>production: SimulationConfig\n</code></pre> <p>Configuration for the production simulation.</p>"},{"location":"reference/targets/thermo/#descent.targets.thermo.SimulationConfig.production_frequency","title":"production_frequency  <code>pydantic-field</code>","text":"<pre><code>production_frequency: int\n</code></pre> <p>The frequency at which to write frames during production.</p>"},{"location":"reference/targets/thermo/#descent.targets.thermo.create_dataset","title":"create_dataset","text":"<pre><code>create_dataset(*rows: DataEntry) -&gt; Dataset\n</code></pre> <p>Create a dataset from a list of existing data points.</p> <p>Parameters:</p> <ul> <li> <code>rows</code>               (<code>DataEntry</code>, default:                   <code>()</code> )           \u2013            <p>The data points to create the dataset from.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Dataset</code>           \u2013            <p>The created dataset.</p> </li> </ul> Source code in <code>descent/targets/thermo.py</code> <pre><code>def create_dataset(*rows: DataEntry) -&gt; datasets.Dataset:\n    \"\"\"Create a dataset from a list of existing data points.\n\n    Args:\n        rows: The data points to create the dataset from.\n\n    Returns:\n        The created dataset.\n    \"\"\"\n\n    for row in rows:\n        row[\"smiles_a\"] = descent.utils.molecule.map_smiles(row[\"smiles_a\"])\n\n        if row[\"smiles_b\"] is None:\n            continue\n\n        row[\"smiles_b\"] = descent.utils.molecule.map_smiles(row[\"smiles_b\"])\n\n    # TODO: validate rows\n    table = pyarrow.Table.from_pylist([*rows], schema=DATA_SCHEMA)\n\n    dataset = datasets.Dataset(datasets.table.InMemoryTable(table))\n    return dataset\n</code></pre>"},{"location":"reference/targets/thermo/#descent.targets.thermo.extract_smiles","title":"extract_smiles","text":"<pre><code>extract_smiles(dataset: Dataset) -&gt; list[str]\n</code></pre> <p>Return a list of unique SMILES strings in the dataset.</p> <p>Parameters:</p> <ul> <li> <code>dataset</code>               (<code>Dataset</code>)           \u2013            <p>The dataset to extract the SMILES strings from.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>list[str]</code>           \u2013            <p>The unique SMILES strings with full atom mapping.</p> </li> </ul> Source code in <code>descent/targets/thermo.py</code> <pre><code>def extract_smiles(dataset: datasets.Dataset) -&gt; list[str]:\n    \"\"\"Return a list of unique SMILES strings in the dataset.\n\n    Args:\n        dataset: The dataset to extract the SMILES strings from.\n\n    Returns:\n        The unique SMILES strings with full atom mapping.\n    \"\"\"\n    smiles_a = {smiles for smiles in dataset.unique(\"smiles_a\") if smiles is not None}\n    smiles_b = {smiles for smiles in dataset.unique(\"smiles_b\") if smiles is not None}\n\n    smiles_unique = sorted({*smiles_a, *smiles_b})\n    return smiles_unique\n</code></pre>"},{"location":"reference/targets/thermo/#descent.targets.thermo.default_config","title":"default_config","text":"<pre><code>default_config(\n    phase: Phase, temperature: float, pressure: float | None\n) -&gt; SimulationConfig\n</code></pre> <p>Return a default simulation configuration for the specified phase.</p> <p>Parameters:</p> <ul> <li> <code>phase</code>               (<code>Phase</code>)           \u2013            <p>The phase to return the default configuration for.</p> </li> <li> <code>temperature</code>               (<code>float</code>)           \u2013            <p>The temperature [K] at which to run the simulation.</p> </li> <li> <code>pressure</code>               (<code>float | None</code>)           \u2013            <p>The pressure [atm] at which to run the simulation.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>SimulationConfig</code>           \u2013            <p>The default simulation configuration.</p> </li> </ul> Source code in <code>descent/targets/thermo.py</code> <pre><code>def default_config(\n    phase: Phase, temperature: float, pressure: float | None\n) -&gt; SimulationConfig:\n    \"\"\"Return a default simulation configuration for the specified phase.\n\n    Args:\n        phase: The phase to return the default configuration for.\n        temperature: The temperature [K] at which to run the simulation.\n        pressure: The pressure [atm] at which to run the simulation.\n\n    Returns:\n        The default simulation configuration.\n    \"\"\"\n\n    if phase.lower() == \"bulk\":\n        return _bulk_config(temperature, pressure)\n    elif phase.lower() == \"vacuum\":\n        return _vacuum_config(temperature, pressure)\n    else:\n        raise NotImplementedError(phase)\n</code></pre>"},{"location":"reference/targets/thermo/#descent.targets.thermo.predict","title":"predict","text":"<pre><code>predict(\n    dataset: Dataset,\n    force_field: TensorForceField,\n    topologies: dict[str, TensorTopology],\n    output_dir: Path,\n    cached_dir: Path | None = None,\n    per_type_scales: dict[DataType, float] | None = None,\n    verbose: bool = False,\n) -&gt; tuple[Tensor, Tensor, Tensor, Tensor]\n</code></pre> <p>Predict the properties in a dataset using molecular simulation, or by reweighting previous simulation data.</p> <p>Parameters:</p> <ul> <li> <code>dataset</code>               (<code>Dataset</code>)           \u2013            <p>The dataset to predict the properties of.</p> </li> <li> <code>force_field</code>               (<code>TensorForceField</code>)           \u2013            <p>The force field to use.</p> </li> <li> <code>topologies</code>               (<code>dict[str, TensorTopology]</code>)           \u2013            <p>The topologies of the molecules present in the dataset, with keys of mapped SMILES patterns.</p> </li> <li> <code>output_dir</code>               (<code>Path</code>)           \u2013            <p>The directory to write the simulation trajectories to.</p> </li> <li> <code>cached_dir</code>               (<code>Path | None</code>, default:                   <code>None</code> )           \u2013            <p>The (optional) directory to read cached simulation trajectories from.</p> </li> <li> <code>per_type_scales</code>               (<code>dict[DataType, float] | None</code>, default:                   <code>None</code> )           \u2013            <p>The scale factor to apply to each data type. A default of 1.0 will be used for any data type not specified.</p> </li> <li> <code>verbose</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to log additional information.</p> </li> </ul> Source code in <code>descent/targets/thermo.py</code> <pre><code>def predict(\n    dataset: datasets.Dataset,\n    force_field: smee.TensorForceField,\n    topologies: dict[str, smee.TensorTopology],\n    output_dir: pathlib.Path,\n    cached_dir: pathlib.Path | None = None,\n    per_type_scales: dict[DataType, float] | None = None,\n    verbose: bool = False,\n) -&gt; tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\n    \"\"\"Predict the properties in a dataset using molecular simulation, or by reweighting\n    previous simulation data.\n\n    Args:\n        dataset: The dataset to predict the properties of.\n        force_field: The force field to use.\n        topologies: The topologies of the molecules present in the dataset, with keys\n            of mapped SMILES patterns.\n        output_dir: The directory to write the simulation trajectories to.\n        cached_dir: The (optional) directory to read cached simulation trajectories\n            from.\n        per_type_scales: The scale factor to apply to each data type. A default of 1.0\n            will be used for any data type not specified.\n        verbose: Whether to log additional information.\n    \"\"\"\n\n    entries: list[DataEntry] = [*descent.utils.dataset.iter_dataset(dataset)]\n\n    required_simulations, entry_to_simulation = _plan_simulations(entries, topologies)\n    observables = {\n        phase: {\n            key: _compute_observables(\n                phase, key, system, force_field, output_dir, cached_dir\n            )\n            for key, system in systems.items()\n        }\n        for phase, systems in required_simulations.items()\n    }\n\n    predicted = []\n    predicted_std = []\n    reference = []\n    reference_std = []\n\n    verbose_rows = []\n\n    per_type_scales = per_type_scales if per_type_scales is not None else {}\n\n    for entry, keys in zip(entries, entry_to_simulation, strict=True):\n        value, std = _predict(entry, keys, observables, required_simulations)\n\n        type_scale = per_type_scales.get(entry[\"type\"], 1.0)\n\n        predicted.append(value * type_scale)\n        predicted_std.append(torch.nan if std is None else std * abs(type_scale))\n\n        reference.append(entry[\"value\"] * type_scale)\n        reference_std.append(\n            torch.nan if entry[\"std\"] is None else entry[\"std\"] * abs(type_scale)\n        )\n\n        if verbose:\n            std_ref = \"\" if entry[\"std\"] is None else f\" \u00b1 {float(entry['std']):.3f}\"\n\n            verbose_rows.append(\n                {\n                    \"type\": f'{entry[\"type\"]} [{entry[\"units\"]}]',\n                    \"smiles_a\": descent.utils.molecule.unmap_smiles(entry[\"smiles_a\"]),\n                    \"smiles_b\": (\n                        \"\"\n                        if entry[\"smiles_b\"] is None\n                        else descent.utils.molecule.unmap_smiles(entry[\"smiles_b\"])\n                    ),\n                    \"pred\": f\"{float(value):.3f} \u00b1 {float(std):.3f}\",\n                    \"ref\": f\"{float(entry['value']):.3f}{std_ref}\",\n                }\n            )\n\n    if verbose:\n        import pandas\n\n        _LOGGER.info(f\"predicted {len(entries)} properties\")\n        _LOGGER.info(\"\\n\" + pandas.DataFrame(verbose_rows).to_string(index=False))\n\n    predicted = torch.stack(predicted)\n    predicted_std = torch.stack(predicted_std)\n\n    reference = smee.utils.tensor_like(reference, predicted)\n    reference_std = smee.utils.tensor_like(reference_std, predicted_std)\n\n    return reference, reference_std, predicted, predicted_std\n</code></pre>"},{"location":"reference/targets/thermo/#descent.targets.thermo.default_closure","title":"default_closure","text":"<pre><code>default_closure(\n    trainable: Trainable,\n    topologies: dict[str, TensorTopology],\n    dataset: Dataset,\n    per_type_scales: dict[DataType, float] | None = None,\n    verbose: bool = False,\n) -&gt; ClosureFn\n</code></pre> <p>Return a default closure function for training against thermodynamic properties.</p> <p>Parameters:</p> <ul> <li> <code>trainable</code>               (<code>Trainable</code>)           \u2013            <p>The wrapper around trainable parameters.</p> </li> <li> <code>topologies</code>               (<code>dict[str, TensorTopology]</code>)           \u2013            <p>The topologies of the molecules present in the dataset, with keys of mapped SMILES patterns.</p> </li> <li> <code>dataset</code>               (<code>Dataset</code>)           \u2013            <p>The dataset to train against.</p> </li> <li> <code>per_type_scales</code>               (<code>dict[DataType, float] | None</code>, default:                   <code>None</code> )           \u2013            <p>The scale factor to apply to each data type.</p> </li> <li> <code>verbose</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to log additional information about predictions.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>ClosureFn</code>           \u2013            <p>The default closure function.</p> </li> </ul> Source code in <code>descent/targets/thermo.py</code> <pre><code>def default_closure(\n    trainable: \"descent.train.Trainable\",\n    topologies: dict[str, smee.TensorTopology],\n    dataset: datasets.Dataset,\n    per_type_scales: dict[DataType, float] | None = None,\n    verbose: bool = False,\n) -&gt; descent.optim.ClosureFn:\n    \"\"\"Return a default closure function for training against thermodynamic\n    properties.\n\n    Args:\n        trainable: The wrapper around trainable parameters.\n        topologies: The topologies of the molecules present in the dataset, with keys\n            of mapped SMILES patterns.\n        dataset: The dataset to train against.\n        per_type_scales: The scale factor to apply to each data type.\n        verbose: Whether to log additional information about predictions.\n\n    Returns:\n        The default closure function.\n    \"\"\"\n\n    def closure_fn(\n        x: torch.Tensor,\n        compute_gradient: bool,\n        compute_hessian: bool,\n    ):\n        force_field = trainable.to_force_field(x)\n\n        y_ref, _, y_pred, _ = descent.targets.thermo.predict(\n            dataset,\n            force_field,\n            topologies,\n            pathlib.Path.cwd(),\n            None,\n            per_type_scales,\n            verbose,\n        )\n        loss, gradient, hessian = ((y_pred - y_ref) ** 2).sum(), None, None\n\n        if compute_hessian:\n            hessian = descent.utils.loss.approximate_hessian(x, y_pred)\n        if compute_gradient:\n            gradient = torch.autograd.grad(loss, x, retain_graph=True)[0].detach()\n\n        return loss.detach(), gradient, hessian\n\n    return closure_fn\n</code></pre>"},{"location":"reference/utils/","title":"Index","text":""},{"location":"reference/utils/#descent.utils","title":"utils","text":"<p>Utilities functions.</p> <p>Modules:</p> <ul> <li> <code>dataset</code>           \u2013            <p>Utilities for working with datasets.</p> </li> <li> <code>loss</code>           \u2013            <p>Utilities for defining loss functions.</p> </li> <li> <code>molecule</code>           \u2013            </li> <li> <code>reporting</code>           \u2013            <p>Utilities for reporting results.</p> </li> </ul>"},{"location":"reference/utils/dataset/","title":"<code class=\"doc-symbol doc-symbol-nav doc-symbol-module\"></code> dataset","text":""},{"location":"reference/utils/dataset/#descent.utils.dataset","title":"dataset","text":"<p>Utilities for working with datasets.</p> <p>Functions:</p> <ul> <li> <code>iter_dataset</code>             \u2013              <p>Iterate over a Hugging Face Dataset, yielding each 'row' as a dictionary.</p> </li> </ul>"},{"location":"reference/utils/dataset/#descent.utils.dataset.iter_dataset","title":"iter_dataset","text":"<pre><code>iter_dataset(dataset: Dataset) -&gt; Iterator[dict[str, Any]]\n</code></pre> <p>Iterate over a Hugging Face Dataset, yielding each 'row' as a dictionary.</p> <p>Parameters:</p> <ul> <li> <code>dataset</code>               (<code>Dataset</code>)           \u2013            <p>The dataset to iterate over.</p> </li> </ul> <p>Yields:</p> <ul> <li> <code>dict[str, Any]</code>           \u2013            <p>A dictionary representing a single entry in the batch, where each key is a</p> </li> <li> <code>dict[str, Any]</code>           \u2013            <p>column name and the corresponding value is the entry in that column for the</p> </li> <li> <code>dict[str, Any]</code>           \u2013            <p>current row.</p> </li> </ul> Source code in <code>descent/utils/dataset.py</code> <pre><code>def iter_dataset(dataset: datasets.Dataset) -&gt; typing.Iterator[dict[str, typing.Any]]:\n    \"\"\"Iterate over a Hugging Face Dataset, yielding each 'row' as a dictionary.\n\n    Args:\n        dataset: The dataset to iterate over.\n\n    Yields:\n        A dictionary representing a single entry in the batch, where each key is a\n        column name and the corresponding value is the entry in that column for the\n        current row.\n    \"\"\"\n\n    columns = [*dataset.features]\n\n    for row in zip(*[dataset[column] for column in columns], strict=True):\n        yield dict(zip(columns, row, strict=True))\n</code></pre>"},{"location":"reference/utils/loss/","title":"<code class=\"doc-symbol doc-symbol-nav doc-symbol-module\"></code> loss","text":""},{"location":"reference/utils/loss/#descent.utils.loss","title":"loss","text":"<p>Utilities for defining loss functions.</p> <p>Functions:</p> <ul> <li> <code>to_closure</code>             \u2013              <p>Convert a loss function to a closure function used by second-order optimizers.</p> </li> <li> <code>combine_closures</code>             \u2013              <p>Combine multiple closures into a single closure.</p> </li> <li> <code>approximate_hessian</code>             \u2013              <p>Compute the outer product approximation of the hessian of a least squares</p> </li> </ul>"},{"location":"reference/utils/loss/#descent.utils.loss.to_closure","title":"to_closure","text":"<pre><code>to_closure(\n    loss_fn: Callable[Concatenate[Tensor, P], Tensor],\n    *args: args,\n    **kwargs: kwargs\n) -&gt; ClosureFn\n</code></pre> <p>Convert a loss function to a closure function used by second-order optimizers.</p> <p>Parameters:</p> <ul> <li> <code>loss_fn</code>               (<code>Callable[Concatenate[Tensor, P], Tensor]</code>)           \u2013            <p>The loss function to convert. This should take in a tensor of parameters with <code>shape=(n,)</code>, and optionally a set of <code>args</code> and <code>kwargs</code>.</p> </li> <li> <code>*args</code>               (<code>args</code>, default:                   <code>()</code> )           \u2013            <p>Positional arguments passed to <code>loss_fn</code>.</p> </li> <li> <code>**kwargs</code>               (<code>kwargs</code>, default:                   <code>{}</code> )           \u2013            <p>Keyword arguments passed to <code>loss_fn</code>.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>ClosureFn</code>           \u2013            <p>A closure function that takes in a tensor of parameters with <code>shape=(n,)</code>, a boolean flag indicating whether to compute the gradient, and a boolean flag indicating whether to compute the Hessian. It returns a tuple of the loss value, the gradient, and the Hessian.</p> </li> </ul> Source code in <code>descent/utils/loss.py</code> <pre><code>def to_closure(\n    loss_fn: typing.Callable[typing.Concatenate[torch.Tensor, P], torch.Tensor],\n    *args: P.args,\n    **kwargs: P.kwargs,\n) -&gt; ClosureFn:\n    \"\"\"Convert a loss function to a closure function used by second-order optimizers.\n\n    Args:\n        loss_fn: The loss function to convert. This should take in a tensor of\n            parameters with ``shape=(n,)``, and optionally a set of ``args`` and\n            ``kwargs``.\n        *args: Positional arguments passed to `loss_fn`.\n        **kwargs: Keyword arguments passed to `loss_fn`.\n\n    Returns:\n        A closure function that takes in a tensor of parameters with ``shape=(n,)``,\n        a boolean flag indicating whether to compute the gradient, and a boolean flag\n        indicating whether to compute the Hessian. It returns a tuple of the loss\n        value, the gradient, and the Hessian.\n    \"\"\"\n\n    loss_fn_wrapped = functools.partial(loss_fn, *args, **kwargs)\n\n    def closure_fn(\n        x: torch.Tensor, compute_gradient: bool, compute_hessian: bool\n    ) -&gt; tuple[torch.Tensor, torch.Tensor | None, torch.Tensor | None]:\n        loss = loss_fn_wrapped(x)\n        gradient, hessian = None, None\n\n        if compute_hessian:\n            hessian = torch.autograd.functional.hessian(\n                loss_fn_wrapped, x, vectorize=True, create_graph=False\n            ).detach()\n        if compute_gradient:\n            (gradient,) = torch.autograd.grad(loss, x, create_graph=False)\n            gradient = gradient.detach()\n\n        return loss.detach(), gradient, hessian\n\n    return closure_fn\n</code></pre>"},{"location":"reference/utils/loss/#descent.utils.loss.combine_closures","title":"combine_closures","text":"<pre><code>combine_closures(\n    closures: dict[str, ClosureFn],\n    weights: dict[str, float] | None = None,\n    verbose: bool = False,\n) -&gt; ClosureFn\n</code></pre> <p>Combine multiple closures into a single closure.</p> <p>Parameters:</p> <ul> <li> <code>closures</code>               (<code>dict[str, ClosureFn]</code>)           \u2013            <p>A dictionary of closure functions.</p> </li> <li> <code>weights</code>               (<code>dict[str, float] | None</code>, default:                   <code>None</code> )           \u2013            <p>Optional dictionary of weights for each closure function.</p> </li> <li> <code>verbose</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to log the loss of each closure function.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>ClosureFn</code>           \u2013            <p>A combined closure function.</p> </li> </ul> Source code in <code>descent/utils/loss.py</code> <pre><code>def combine_closures(\n    closures: dict[str, ClosureFn],\n    weights: dict[str, float] | None = None,\n    verbose: bool = False,\n) -&gt; ClosureFn:\n    \"\"\"Combine multiple closures into a single closure.\n\n    Args:\n        closures: A dictionary of closure functions.\n        weights: Optional dictionary of weights for each closure function.\n        verbose: Whether to log the loss of each closure function.\n\n    Returns:\n        A combined closure function.\n    \"\"\"\n\n    weights = weights if weights is not None else {name: 1.0 for name in closures}\n\n    if len(closures) == 0:\n        raise NotImplementedError(\"At least one closure function is required.\")\n\n    if {*closures} != {*weights}:\n        raise ValueError(\"The closures and weights must have the same keys.\")\n\n    def combined_closure_fn(\n        x: torch.Tensor, compute_gradient: bool, compute_hessian: bool\n    ) -&gt; tuple[torch.Tensor, torch.Tensor | None, torch.Tensor | None]:\n        loss = []\n        grad = None if not compute_gradient else []\n        hess = None if not compute_hessian else []\n\n        verbose_rows = []\n\n        for name, closure_fn in closures.items():\n            local_loss, local_grad, local_hess = closure_fn(\n                x, compute_gradient, compute_hessian\n            )\n\n            loss.append(weights[name] * local_loss)\n\n            if compute_gradient:\n                grad.append(weights[name] * local_grad)\n            if compute_hessian:\n                hess.append(weights[name] * local_hess)\n\n            if verbose:\n                verbose_rows.append(\n                    {\"target\": name, \"loss\": float(f\"{local_loss:.5f}\")}\n                )\n\n        loss = sum(loss[1:], loss[0])\n\n        if compute_gradient:\n            grad = sum(grad[1:], grad[0]).detach()\n        if compute_hessian:\n            hess = sum(hess[1:], hess[0]).detach()\n\n        if verbose:\n            import pandas\n\n            _LOGGER.info(\n                \"loss breakdown:\\n\"\n                + pandas.DataFrame(verbose_rows).to_string(index=False)\n            )\n\n        return loss.detach(), grad, hess\n\n    return combined_closure_fn\n</code></pre>"},{"location":"reference/utils/loss/#descent.utils.loss.approximate_hessian","title":"approximate_hessian","text":"<pre><code>approximate_hessian(x: Tensor, y_pred: Tensor)\n</code></pre> <p>Compute the outer product approximation of the hessian of a least squares loss function of the sum <code>sum((y_pred - y_ref)**2)</code>.</p> <p>Parameters:</p> <ul> <li> <code>x</code>               (<code>Tensor</code>)           \u2013            <p>The parameter tensor with <code>shape=(n_parameters,)</code>.</p> </li> <li> <code>y_pred</code>               (<code>Tensor</code>)           \u2013            <p>The values predicted using <code>x</code> with <code>shape=(n_predications,)</code>.</p> </li> </ul> <p>Returns:</p> <ul> <li>           \u2013            <p>The outer product approximation of the hessian with ``shape=n_parameters</p> </li> </ul> Source code in <code>descent/utils/loss.py</code> <pre><code>def approximate_hessian(x: torch.Tensor, y_pred: torch.Tensor):\n    \"\"\"Compute the outer product approximation of the hessian of a least squares\n    loss function of the sum ``sum((y_pred - y_ref)**2)``.\n\n    Args:\n        x: The parameter tensor with ``shape=(n_parameters,)``.\n        y_pred: The values predicted using ``x`` with ``shape=(n_predications,)``.\n\n    Returns:\n        The outer product approximation of the hessian with ``shape=n_parameters\n    \"\"\"\n\n    y_pred_grad = [torch.autograd.grad(y, x, retain_graph=True)[0] for y in y_pred]\n    y_pred_grad = torch.stack(y_pred_grad, dim=0)\n\n    return (\n        2.0 * torch.einsum(\"bi,bj-&gt;bij\", y_pred_grad, y_pred_grad).sum(dim=0)\n    ).detach()\n</code></pre>"},{"location":"reference/utils/molecule/","title":"<code class=\"doc-symbol doc-symbol-nav doc-symbol-module\"></code> molecule","text":""},{"location":"reference/utils/molecule/#descent.utils.molecule","title":"molecule","text":"<p>Functions:</p> <ul> <li> <code>mol_to_smiles</code>             \u2013              <p>Convert a molecule to a SMILES string with atom mapping.</p> </li> <li> <code>unmap_smiles</code>             \u2013              <p>Remove atom mapping from a SMILES string.</p> </li> <li> <code>map_smiles</code>             \u2013              <p>Add atom mapping to a SMILES string.</p> </li> </ul>"},{"location":"reference/utils/molecule/#descent.utils.molecule.mol_to_smiles","title":"mol_to_smiles","text":"<pre><code>mol_to_smiles(mol: Mol, canonical: bool = True) -&gt; str\n</code></pre> <p>Convert a molecule to a SMILES string with atom mapping.</p> <p>Parameters:</p> <ul> <li> <code>mol</code>               (<code>Mol</code>)           \u2013            <p>The molecule to convert.</p> </li> <li> <code>canonical</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to canonicalize the atom ordering prior to assigning map indices.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>str</code>           \u2013            <p>The SMILES string.</p> </li> </ul> Source code in <code>descent/utils/molecule.py</code> <pre><code>def mol_to_smiles(mol: \"Chem.Mol\", canonical: bool = True) -&gt; str:\n    \"\"\"Convert a molecule to a SMILES string with atom mapping.\n\n    Args:\n        mol: The molecule to convert.\n        canonical: Whether to canonicalize the atom ordering prior to assigning\n            map indices.\n\n    Returns:\n        The SMILES string.\n    \"\"\"\n    from rdkit import Chem\n\n    mol = Chem.AddHs(mol)\n\n    if canonical:\n        order = Chem.CanonicalRankAtoms(mol, includeChirality=True)\n        mol = Chem.RenumberAtoms(mol, list(order))\n\n    for atom in mol.GetAtoms():\n        atom.SetAtomMapNum(atom.GetIdx() + 1)\n\n    return Chem.MolToSmiles(mol)\n</code></pre>"},{"location":"reference/utils/molecule/#descent.utils.molecule.unmap_smiles","title":"unmap_smiles","text":"<pre><code>unmap_smiles(smiles: str) -&gt; str\n</code></pre> <p>Remove atom mapping from a SMILES string.</p> <p>Parameters:</p> <ul> <li> <code>smiles</code>               (<code>str</code>)           \u2013            <p>The SMILES string to unmap.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>str</code>           \u2013            <p>The unmapped SMILES string.</p> </li> </ul> Source code in <code>descent/utils/molecule.py</code> <pre><code>def unmap_smiles(smiles: str) -&gt; str:\n    \"\"\"Remove atom mapping from a SMILES string.\n\n    Args:\n        smiles: The SMILES string to unmap.\n\n    Returns:\n        The unmapped SMILES string.\n    \"\"\"\n    from rdkit import Chem\n\n    mol = Chem.MolFromSmiles(smiles)\n\n    for atom in mol.GetAtoms():\n        atom.SetAtomMapNum(0)\n\n    return Chem.MolToSmiles(mol)\n</code></pre>"},{"location":"reference/utils/molecule/#descent.utils.molecule.map_smiles","title":"map_smiles","text":"<pre><code>map_smiles(smiles: str) -&gt; str\n</code></pre> <p>Add atom mapping to a SMILES string.</p> Notes <p>Fully mapped SMILES strings are returned as-is.</p> <p>Parameters:</p> <ul> <li> <code>smiles</code>               (<code>str</code>)           \u2013            <p>The SMILES string to add map indices to.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>str</code>           \u2013            <p>The mapped SMILES string.</p> </li> </ul> Source code in <code>descent/utils/molecule.py</code> <pre><code>def map_smiles(smiles: str) -&gt; str:\n    \"\"\"Add atom mapping to a SMILES string.\n\n    Notes:\n        Fully mapped SMILES strings are returned as-is.\n\n    Args:\n        smiles: The SMILES string to add map indices to.\n\n    Returns:\n        The mapped SMILES string.\n    \"\"\"\n    from rdkit import Chem\n\n    params = Chem.SmilesParserParams()\n    params.removeHs = False\n\n    mol = Chem.AddHs(Chem.MolFromSmiles(smiles, params))\n\n    map_idxs = sorted(atom.GetAtomMapNum() for atom in mol.GetAtoms())\n\n    if map_idxs == list(range(1, len(map_idxs) + 1)):\n        return smiles\n\n    for i, atom in enumerate(mol.GetAtoms()):\n        atom.SetAtomMapNum(i + 1)\n\n    return Chem.MolToSmiles(mol)\n</code></pre>"},{"location":"reference/utils/reporting/","title":"<code class=\"doc-symbol doc-symbol-nav doc-symbol-module\"></code> reporting","text":""},{"location":"reference/utils/reporting/#descent.utils.reporting","title":"reporting","text":"<p>Utilities for reporting results.</p> <p>Functions:</p> <ul> <li> <code>mols_to_img</code>             \u2013              <p>Renders a set of molecules as an embeddable HTML image tag.</p> </li> <li> <code>figure_to_img</code>             \u2013              <p>Convert a matplotlib figure to an embeddable HTML image tag.</p> </li> <li> <code>print_potential_summary</code>             \u2013              <p>Print a summary of the potential parameters to the terminal.</p> </li> <li> <code>print_force_field_summary</code>             \u2013              <p>Print a summary of the force field parameters to the terminal.</p> </li> </ul>"},{"location":"reference/utils/reporting/#descent.utils.reporting.mols_to_img","title":"mols_to_img","text":"<pre><code>mols_to_img(\n    *smiles: str, width: int = 400, height: int = 200\n) -&gt; str\n</code></pre> <p>Renders a set of molecules as an embeddable HTML image tag.</p> <p>Parameters:</p> <ul> <li> <code>*smiles</code>               (<code>str</code>, default:                   <code>()</code> )           \u2013            <p>The SMILES patterns of the molecules to render.</p> </li> <li> <code>width</code>               (<code>int</code>, default:                   <code>400</code> )           \u2013            <p>The width of the image.</p> </li> <li> <code>height</code>               (<code>int</code>, default:                   <code>200</code> )           \u2013            <p>The height of the image.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>str</code>           \u2013            <p>The HTML image tag.</p> </li> </ul> Source code in <code>descent/utils/reporting.py</code> <pre><code>def mols_to_img(*smiles: str, width: int = 400, height: int = 200) -&gt; str:\n    \"\"\"Renders a set of molecules as an embeddable HTML image tag.\n\n    Args:\n        *smiles: The SMILES patterns of the molecules to render.\n        width: The width of the image.\n        height: The height of the image.\n\n    Returns:\n        The HTML image tag.\n    \"\"\"\n    from rdkit import Chem\n    from rdkit.Chem import Draw\n\n    assert len(smiles) &gt; 0\n\n    mol = _mol_from_smiles(smiles[0])\n\n    for pattern in smiles[1:]:\n        mol = Chem.CombineMols(mol, _mol_from_smiles(pattern))\n\n    mol = Draw.PrepareMolForDrawing(mol, forceCoords=True)\n\n    drawer = Draw.rdMolDraw2D.MolDraw2DSVG(width, height)\n    drawer.DrawMolecule(mol)\n    drawer.FinishDrawing()\n\n    data = base64.b64encode(drawer.GetDrawingText().encode()).decode()\n    return f'&lt;img src=\"data:image/svg+xml;base64,{data}\"&gt;&lt;/img&gt;'\n</code></pre>"},{"location":"reference/utils/reporting/#descent.utils.reporting.figure_to_img","title":"figure_to_img","text":"<pre><code>figure_to_img(figure: Figure) -&gt; str\n</code></pre> <p>Convert a matplotlib figure to an embeddable HTML image tag.</p> <p>Parameters:</p> <ul> <li> <code>figure</code>               (<code>Figure</code>)           \u2013            <p>The figure to convert.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>str</code>           \u2013            <p>The HTML image tag.</p> </li> </ul> Source code in <code>descent/utils/reporting.py</code> <pre><code>def figure_to_img(figure: \"pyplot.Figure\") -&gt; str:\n    \"\"\"Convert a matplotlib figure to an embeddable HTML image tag.\n\n    Args:\n        figure: The figure to convert.\n\n    Returns:\n        The HTML image tag.\n    \"\"\"\n\n    with io.BytesIO() as stream:\n        figure.savefig(stream, format=\"svg\")\n        data = base64.b64encode(stream.getvalue()).decode()\n\n    return f'&lt;img src=\"data:image/svg+xml;base64,{data}\"&gt;&lt;/img&gt;'\n</code></pre>"},{"location":"reference/utils/reporting/#descent.utils.reporting.print_potential_summary","title":"print_potential_summary","text":"<pre><code>print_potential_summary(potential: TensorPotential)\n</code></pre> <p>Print a summary of the potential parameters to the terminal.</p> <p>Parameters:</p> <ul> <li> <code>potential</code>               (<code>TensorPotential</code>)           \u2013            <p>The potential.</p> </li> </ul> Source code in <code>descent/utils/reporting.py</code> <pre><code>def print_potential_summary(potential: smee.TensorPotential):\n    \"\"\"Print a summary of the potential parameters to the terminal.\n\n    Args:\n        potential: The potential.\n    \"\"\"\n    import pandas\n\n    parameter_rows = []\n\n    for key, value in zip(\n        potential.parameter_keys, potential.parameters.detach(), strict=True\n    ):\n        row = {\"ID\": _format_parameter_id(key.id)}\n        row.update(\n            {\n                f\"{col}{_format_unit(potential.parameter_units[idx])}\": (\n                    f\"{value[idx].item():.4f}\"\n                )\n                for idx, col in enumerate(potential.parameter_cols)\n            }\n        )\n        parameter_rows.append(row)\n\n    print(f\" {potential.type} \".center(88, \"=\"), flush=True)\n    print(f\"fn={potential.fn}\", flush=True)\n\n    if potential.attributes is not None:\n        attribute_rows = [\n            {\n                f\"{col}{_format_unit(potential.attribute_units[idx])}\": (\n                    f\"{potential.attributes[idx].item():.4f} \"\n                )\n                for idx, col in enumerate(potential.attribute_cols)\n            }\n        ]\n        print(\"\")\n        print(\"attributes=\", flush=True)\n        print(\"\")\n        print(pandas.DataFrame(attribute_rows).to_string(index=False), flush=True)\n\n    print(\"\")\n    print(\"parameters=\", flush=True)\n    print(\"\")\n    print(pandas.DataFrame(parameter_rows).to_string(index=False), flush=True)\n</code></pre>"},{"location":"reference/utils/reporting/#descent.utils.reporting.print_force_field_summary","title":"print_force_field_summary","text":"<pre><code>print_force_field_summary(force_field: TensorForceField)\n</code></pre> <p>Print a summary of the force field parameters to the terminal.</p> <p>Parameters:</p> <ul> <li> <code>force_field</code>               (<code>TensorForceField</code>)           \u2013            <p>The force field.</p> </li> </ul> Source code in <code>descent/utils/reporting.py</code> <pre><code>def print_force_field_summary(force_field: smee.TensorForceField):\n    \"\"\"Print a summary of the force field parameters to the terminal.\n\n    Args:\n        force_field: The force field.\n    \"\"\"\n\n    if force_field.v_sites is not None:\n        print_v_site_summary(force_field.v_sites)\n        print(\"\")\n\n    for potential in force_field.potentials:\n        print_potential_summary(potential)\n        print(\"\")\n</code></pre>"}]}